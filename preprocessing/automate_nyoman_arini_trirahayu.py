# -*- coding: utf-8 -*-
"""automate_nyoman_arini_trirahayu

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NHzJEV-vqQ1Rzyx1HhK1sbo_Ohbm0qAT

# **1. load_data(path)**

# Pengenalan Dataset: Bank Customer Churn

Dataset yang digunakan berasal dari **Bank Customer Churn Dataset**, yaitu kumpulan data pelanggan bank yang bertujuan untuk memprediksi apakah seorang pelanggan akan berhenti menggunakan layanan bank (churn) atau tetap aktif. Dataset ini sering digunakan untuk **analisis retensi pelanggan**, **machine learning supervised classification**, serta **eksperimen MLOps** karena memiliki kombinasi fitur numerik dan kategorikal yang seimbang.  

Dataset ini bersifat publik dan banyak digunakan untuk keperluan edukasi dan eksperimen prediksi churn. Dataset mencerminkan perilaku nyata pelanggan bank, walaupun data ini disediakan dalam bentuk fiktif.

### ðŸŽ¯ Tujuan Penggunaan Dataset
- Menganalisis karakteristik pelanggan bank.  
- Memahami faktor-faktor yang mempengaruhi churn.  
- Mengembangkan model prediksi churn menggunakan machine learning.

### ðŸ§® Jumlah Data
- **Baris (Rows):** 10.000 pelanggan (contoh; sesuaikan jika berbeda)  
- **Kolom (Features):** 14 fitur + 1 target (Churn)

### ðŸ§© Deskripsi Kolom

| Fitur | Tipe | Deskripsi |
|-------|------|-----------|
| RowNumber | int | Nomor baris pelanggan |
| CustomerId | object | ID unik pelanggan |
| Surname | object | Nama belakang pelanggan |
| CreditScore | int | Skor kredit pelanggan |
| Geography | categorical | Negara/Daerah pelanggan |
| Gender | categorical | Jenis kelamin pelanggan |
| Age | int | Usia pelanggan |
| Tenure | int | Lama menjadi nasabah (tahun) |
| Balance | float | Saldo rekening nasabah |
| NumOfProducts | int | Jumlah produk yang dimiliki |
| HasCrCard | int | Memiliki kartu kredit (0/1) |
| IsActiveMember | int | Status keaktifan nasabah (0/1) |
| EstimatedSalary | float | Perkiraan penghasilan tahunan |
| Exaited | categorical | Target prediksi: 1 = churn, 0 = tetap |

# 1. Import Library

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
import os

"""# **3. save_preprocessed(df, path)**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

# ======= CONFIGURATION =======
# Folder output preprocessing
PREPROCESS_DIR = os.path.join("preprocessing", "DatasetBank_preprocessing")
# Nama file output hasil preprocessing
OUTPUT_FILE_NAME = "churn_preprocessed.csv"

# Link raw dataset GitHub
RAW_URL = "https://raw.githubusercontent.com/nyomanarinit/Eksperimen_SML_Nyoman-Arini-Trirahayu/refs/heads/main/Churn_Modelling.csv"

# Pastikan folder preprocessing ada
os.makedirs(PREPROCESS_DIR, exist_ok=True)

# ======= LOAD DATASET =======
print(f"[INFO] Memuat dataset dari GitHub raw URL...")
df = pd.read_csv(RAW_URL)
print(f"[INFO] Dataset dimuat: {df.shape[0]} baris x {df.shape[1]} kolom")

# ======= DROP DUPLICATES =======
df = df.drop_duplicates()
print(f"[INFO] Setelah drop duplicates: {df.shape[0]} baris x {df.shape[1]} kolom")

# ======= DROP KOLON ID/UNIK =======
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# ======= ENCODE KATEGORICAL =======
categorical_cols = df.select_dtypes(include=['object']).columns
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])
print(f"[INFO] Kolom kategorikal berhasil di-encode: {list(categorical_cols)}")

# ======= STANDARDISASI NUMERIK =======
from sklearn.preprocessing import StandardScaler

# Kolom numerik yang tidak boleh di-scale
exclude_cols = ['Exited']  # sesuaikan jika dataset kamu berbeda
# Pilih kolom numerik murni (bukan kategori yang di-encode)
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
# Standardisasi
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
print(f"[INFO] Kolom numerik berhasil distandardisasi: {numeric_cols}")


# ======= SIMPAN HASIL PREPROCESSING =======
output_path = os.path.join(PREPROCESS_DIR, OUTPUT_FILE_NAME)
df.to_csv(output_path, index=False)
print(f"[INFO] Dataset hasil preprocessing tersimpan di: {output_path}")

import pandas as pd
df = pd.read_csv("./preprocessing/DatasetBank_preprocessing/churn_preprocessed.csv")
df.head()